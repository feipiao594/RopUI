# 跨平台调度器开发

这个阶段的任务主要是实现各个平台的 eventloop_core 用来提供不同平台的监听事件

目前 Linux 平台已经完整支持

Windows/MacOS 平台需要测试

严重警告：windows 平台的 api 依赖一个不可以重复的 key 值，需要在后期加入一个全局 key 池来避免 key 碰撞，非常注意，wakeup 目前硬编码了这个 key 值，后期一定一定要改(未必，可能不会发生哈希碰撞，需接着检查，暂时不处理)

注意 watcher 的设计，就是被回掉捕获的 watcher 持有的资源需要 shared_ptr 包一层，因为能不捕获 this 就不捕获（优先 State 或把资源/逻辑放进 source）。例子如 wakeupWatcher 系列
只有当确实没办法要捕获 this 时，才用 weak_ptr 兜底（并把 watcher 做成 shared_ptr 生命周期）。例子如 example/tcp_client_example


macos:
```objectivec
NSEvent* ev = [NSEvent otherEventWithType:NSEventTypeApplicationDefined
                                 location:NSZeroPoint
                            modifierFlags:0
                                timestamp:0
                             windowNumber:0
                                  context:nil
                                  subtype:0
                                    data1:0
                                    data2:0];
```

这边我们规定了 wakeup 自定义事件的 subtype 是 0

## Work-Stealing 可视化（自写解析器版本）

目标：把调度器关键事件以“可解析的一行日志”输出到文本文件，再用 Python 离线解析/生成图表。

### 1) Trace 日志格式
调度器会在 `ROP_SCHED_TRACE=1` 时输出形如：

`SCHED|<ts_ns>|<worker_id>|<event>|k=v k=v ...`

目前的关键事件：
- `local_push`：向本 worker 的 local deque 投递（做了采样输出）。
- `local_degrade_global`：local 满或非 owner 线程调用，退化到 global。
- `steal_success`：窃取成功（含 victim/stolen/local/victim_local）。

### 2) 触发 stealing 的可重复场景
仓库内置了一个 demo：`example/work_steal_trace_demo`。
- 用 `Hive::postToWorker(0, ...)` 保证“生产任务”的种子在 worker0 上跑。
- 种子任务在 worker0 上通过 `IOWorker::postToLocal(...)` 生成大量本地 backlog。
- 其他 worker 通过短周期 timer 保持唤醒，从而在 global 空时会进入 Step-6 去 steal。

### 3) 解析与可视化
解析脚本：`tools/sched_trace/parse_sched_trace.py`
- 输入：日志文件（或 stdin）
- 输出：按 worker 汇总 stealing 数据；可选生成一个只画 `steal_success` 的简易 SVG 时间线。

动画回放（离线 HTML）：`tools/sched_trace/gen_sched_trace_anim.py`
- 输出：一个本地 `html` 文件，用浏览器打开即可播放（不需要跑 webconsole，更不会被调度器影响）。

示例流程：
- 构建：`cmake -S . -B build -DROPUI_BUILD_EXAMPLES=ON && cmake --build build -j`
- 运行并输出日志（二选一）：
  - 环境变量：`ROP_SCHED_TRACE=1 ./build/example/work_steal_trace_demo/work_steal_trace_demo --log trace.log`
  - 启动参数：`./build/example/work_steal_trace_demo/work_steal_trace_demo --sched-trace --log trace.log`
- 解析并生成 SVG：`python3 tools/sched_trace/parse_sched_trace.py trace.log --svg trace.svg`
- 生成动画 HTML：`python3 tools/sched_trace/gen_sched_trace_anim.py trace.log -o trace_anim.html`



在 64 位操作系统上，现有的双向队列自程序启动到正常使用最大上限的偷取次数是 size_t 的最大值，在某些 32 位操作系统上由于该特性进程可能在不到一年内因达到该上限溢出而导致偷取任务这个行为完全失败，但程序应该会降级成普通 eventloop，64 位操作系统的话足够该进程用到天荒地老了
后续可能需要修复这个问题，直接改 uint64_t 就好了


以后可能要创建一些task,可以挂载一个 workerwatcher 到本地 worker,参考下面这个方案
```cpp
#include "platform/linux/schedule/watcher/epoll_worker_timer.h"
#include "platform/schedule/hive.h"
#include "platform/schedule/io_worker.h"
#include "platform/schedule/worker_watcher.h"

#include <iostream>
#include <memory>

static uint64_t test_count = 0;

int main() {
    logger::setMinLevel(LogLevel::DEBUG);
    RopHive::Hive hive;
    auto worker = std::make_shared<RopHive::IOWorker>(hive.options());
    hive.attachIOWorker(worker);

    hive.postIO([]() {
        using watcher_ptr = std::shared_ptr<RopHive::Linux::EpollWorkerTimerWatcher>;
        auto* self = RopHive::IOWorker::currentWorker();
        if (!self) return;

        auto watcher_ptr_ = std::make_shared<watcher_ptr>();

        auto cb = [watcher_ptr_]() {
            std::cout << "postIO(timerfd) " << ++test_count << std::endl;
            if (test_count < 5) return;
            if (watcher_ptr_ && *watcher_ptr_) {
                (*watcher_ptr_)->clearItimerspec();
                (*watcher_ptr_)->stop();
            } else {
                return;
            }
            (*watcher_ptr_).reset();

            if (auto* w = RopHive::IOWorker::currentWorker()) {
                w->requestStop();
            }
        };

        auto watcher = std::make_shared<RopHive::Linux::EpollWorkerTimerWatcher>(*self, std::move(cb));
        *watcher_ptr_ = watcher;

        watcher->setItimerspec(itimerspec{{1, 0}, {2, 0}});
        watcher->start();
    });

    hive.run();
    return 0;
}
```

那么这个就是一个完整的在本地worker创建watcher的方案了
我想要这个的目的是，考虑下面的场景，有一个tcp服务器，如果他运行在我所想要的多线程hive场景下，一般会有一个accept任务，这个accept任务会产生connection,connection会放进global池子随机分配给所有worker,如果是ui线程他不能处理这个任务，就把task重投到global,如果是能处理这个任务的，就用现在timefd在自己worker创建watcher的这种方式创建tcpconnectionwatcher,这样就可以在多线程场景下高性能处理tcp连接任务了
最终权衡结果：重投机制不能选，会严重降低性能；后续考虑给 task 增加“可运行所需 tag/能力标记”，只在进入全局池时标记/分桶即可，暂时不处理。